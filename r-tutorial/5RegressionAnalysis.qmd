---
title: "5 Regression Analysis and Time Series Forecasting"
author: "Mohan Khanal"
date: "2026-01-24"
format:
  html:
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 2
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(forecast)
library(ISLR)
options(warn = -1)
```

# üìà Chapter 5: Regression Analysis and Time Series Forecasting

## üïê Agenda

-   **Part 1 (40 mins)** ‚Äì Linear Regression (Predicting Continuous Outcomes)
-   **Part 2 (30 mins)** ‚Äì Logistic Regression (Predicting Binary Outcomes)
-   **Part 3 (30 mins)** ‚Äì ARIMA Time Series Forecasting
-   **Part 4 (10 mins)** ‚Äì Model Comparison and Selection
-   **Part 5 (10 mins)** ‚Äì Practice and Q&A

------------------------------------------------------------------------

## Setup: Load Libraries and Data

```{r, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(ggplot2)
library(stats)      # For lm(), glm()
library(datasets)   # For mtcars, AirPassengers
library(forecast)   # For ARIMA modeling
library(ISLR)       # For Credit dataset

# Load datasets
data(mtcars)
data(Credit)
data(AirPassengers)
```

------------------------------------------------------------------------

## Part 1: Linear Regression (40 mins)

### What is Linear Regression?

Linear regression models the relationship between:
- **Dependent variable** (outcome we want to predict)
- **Independent variable(s)** (predictors we use)

**Example Question:** Can we predict a car's fuel efficiency (mpg) based on its horsepower, weight, and transmission type?

### Exploring the Data

```{r}
# Preview mtcars dataset
head(mtcars)

# Summary statistics
summary(mtcars[, c("mpg", "hp", "wt", "am")])

# Check structure
str(mtcars)
```

**Key Variables:**
- `mpg`: Miles per gallon (fuel efficiency)
- `hp`: Horsepower
- `wt`: Weight (1000 lbs)
- `am`: Transmission (0 = automatic, 1 = manual)

### Visualize Relationships

```{r}
# Scatter plot: Weight vs MPG
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(
    title = "Relationship Between Car Weight and Fuel Efficiency",
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon"
  ) +
  theme_minimal()
```

### Correlation Check

```{r}
# Correlation matrix
cor(mtcars[, c("mpg", "hp", "wt", "am")])
```

### Building the Linear Regression Model

```{r}
# Fit linear regression model
lin_reg <- lm(mpg ~ hp + wt + am, data = mtcars)

# Display model summary
summary(lin_reg)
```

### Interpreting Linear Regression Output

**Key Components:**

1. **Coefficients:**
   - Shows the effect of each predictor on mpg
   - **Positive coefficient**: Variable increases, mpg increases
   - **Negative coefficient**: Variable increases, mpg decreases

2. **p-value (Pr(>|t|)):**
   - p < 0.05: Predictor is **statistically significant**
   - Use *** (p < 0.001), ** (p < 0.01), * (p < 0.05) as guides

3. **R-squared:**
   - Proportion of variance explained (0 to 1)
   - Higher = better fit
   - Example: R¬≤ = 0.84 means model explains 84% of variance

4. **Adjusted R-squared:**
   - Adjusted for number of predictors
   - Use for comparing models with different numbers of predictors

**üí° Example Interpretation:**
"For every 1000 lb increase in weight, mpg decreases by 3.88, holding horsepower and transmission constant (p < 0.001)."

### Making Predictions

```{r}
# Create new data for prediction
new_cars <- data.frame(
  hp = c(100, 150, 200),
  wt = c(2.5, 3.0, 3.5),
  am = c(1, 1, 0)
)

# Predict mpg
predictions <- predict(lin_reg, newdata = new_cars, interval = "confidence")
predictions

# Combine with input data
cbind(new_cars, predictions)
```

### Model Diagnostics

```{r}
# Diagnostic plots
par(mfrow = c(2, 2))
plot(lin_reg)
```

**What to look for:**
1. **Residuals vs Fitted**: Should show random scatter (no pattern)
2. **Q-Q Plot**: Points should follow line (normality check)
3. **Scale-Location**: Should show random scatter (equal variance)
4. **Residuals vs Leverage**: Identifies influential points

### Visualizing the Model

```{r}
# Scatter plot with regression line by transmission
ggplot(mtcars, aes(x = wt, y = mpg, color = as.factor(am))) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    title = "MPG vs Weight by Transmission Type",
    x = "Weight (1000 lbs)",
    y = "Miles per Gallon",
    color = "Transmission"
  ) +
  scale_color_manual(
    values = c("red", "blue"),
    labels = c("Automatic", "Manual")
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

## Part 2: Logistic Regression (30 mins)

### What is Logistic Regression?

Logistic regression predicts **binary outcomes** (Yes/No, 0/1, True/False).

**Example Question:** Can we predict whether someone is married based on their income, credit balance, and age?

### Preparing the Data

```{r}
# Load and prepare Credit dataset
credit_data <- Credit

# Create binary target variable
credit_data$marriedTarget <- ifelse(credit_data$Married == "Yes", 1, 0)

# Preview data
head(credit_data[, c("Income", "Balance", "Age", "Married", "marriedTarget")])

# Summary
summary(credit_data[, c("Income", "Balance", "Age", "marriedTarget")])
```

### Visualize the Data

```{r}
# Distribution of married status
ggplot(credit_data, aes(x = Married, fill = Married)) +
  geom_bar() +
  labs(
    title = "Distribution of Marital Status",
    x = "Married",
    y = "Count"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
# Income by marital status
ggplot(credit_data, aes(x = Married, y = Income, fill = Married)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Income Distribution by Marital Status",
    x = "Married",
    y = "Income ($1000s)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

### Building the Logistic Regression Model

```{r}
# Fit logistic regression model
log_reg <- glm(marriedTarget ~ Income + Balance + Age, 
               family = binomial(link = "logit"), 
               data = credit_data)

# Display model summary
summary(log_reg)
```

### Interpreting Logistic Regression Output

**Key Components:**

1. **Coefficients:**
   - Represent change in **log-odds** of outcome = 1
   - **Positive coefficient**: Predictor increases probability
   - **Negative coefficient**: Predictor decreases probability

2. **p-value (Pr(>|z|)):**
   - p < 0.05: Predictor is statistically significant

3. **AIC (Akaike Information Criterion):**
   - Lower AIC = better model
   - Use for comparing different models

4. **Deviance:**
   - Lower residual deviance = better fit

**üí° Example Interpretation:**
"Each $1000 increase in income increases the log-odds of being married by 0.004 (p < 0.05)."

### Converting to Odds Ratios

```{r}
# Calculate odds ratios (exponentiate coefficients)
odds_ratios <- exp(coef(log_reg))
odds_ratios

# With confidence intervals
exp(cbind(OR = coef(log_reg), confint(log_reg)))
```

**Interpreting Odds Ratios:**
- OR > 1: Predictor increases odds of outcome
- OR < 1: Predictor decreases odds of outcome
- OR = 1: No effect

### Making Predictions

```{r}
# Predict probability for new observations
new_people <- data.frame(
  Income = c(30, 50, 70),
  Balance = c(500, 1000, 1500),
  Age = c(25, 35, 45)
)

# Predict probabilities
predicted_probs <- predict(log_reg, newdata = new_people, type = "response")

# Combine with input
result <- cbind(new_people, Prob_Married = predicted_probs)
result
```

### Visualizing Predictions

```{r}
# Add predicted probabilities to original data
credit_data$pred_prob <- predict(log_reg, type = "response")

# Plot predicted probabilities by age
ggplot(credit_data, aes(x = Age, y = pred_prob, color = Gender)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = TRUE) +
  labs(
    title = "Predicted Probability of Being Married",
    subtitle = "By Age and Gender",
    x = "Age",
    y = "Probability of Being Married"
  ) +
  theme_minimal()
```

### Model Evaluation

```{r}
# Create confusion matrix (threshold = 0.5)
credit_data$predicted <- ifelse(credit_data$pred_prob > 0.5, 1, 0)

# Confusion matrix
table(Predicted = credit_data$predicted, Actual = credit_data$marriedTarget)

# Calculate accuracy
accuracy <- mean(credit_data$predicted == credit_data$marriedTarget)
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
```

------------------------------------------------------------------------

## Part 3: ARIMA Time Series Forecasting (30 mins)

### What is ARIMA?

**ARIMA** = AutoRegressive Integrated Moving Average

Used for forecasting time series data based on past patterns.

**Example Question:** Can we forecast airline passenger numbers for the next 12 months?

### Understanding the Data

```{r}
# Preview AirPassengers data
head(AirPassengers)
tail(AirPassengers)

# Summary
summary(AirPassengers)

# Check frequency and time range
frequency(AirPassengers)
start(AirPassengers)
end(AirPassengers)
```

### Visualize Time Series

```{r}
# Basic time series plot
plot(AirPassengers, 
     main = "Monthly Airline Passengers (1949-1960)",
     ylab = "Number of Passengers",
     xlab = "Year",
     col = "blue",
     lwd = 2)
```

### Decompose Time Series

```{r}
# Decompose into trend, seasonal, and random components
decomposed <- decompose(AirPassengers)
plot(decomposed)
```

**Components:**
- **Trend**: Long-term increase/decrease
- **Seasonal**: Regular patterns (monthly, yearly)
- **Random**: Irregular fluctuations

### Building ARIMA Model

```{r}
# Auto-select best ARIMA model
arima_model <- auto.arima(AirPassengers)

# Display model summary
summary(arima_model)
```

### Understanding ARIMA(p,d,q)

**Parameters:**
- **p** (AR): Number of autoregressive terms (past values)
- **d** (I): Number of differences to make series stationary
- **q** (MA): Number of moving average terms (past errors)

**Example:** ARIMA(2,1,1) means:
- Uses 2 lagged observations
- Takes 1st difference
- Uses 1 lagged forecast error

### Making Forecasts

```{r}
# Forecast next 12 months
arima_forecast <- forecast(arima_model, h = 12)

# Display forecast
print(arima_forecast)
```

### Visualize Forecast

```{r}
# Plot forecast with confidence intervals
plot(arima_forecast,
     main = "ARIMA Forecast for AirPassengers",
     xlab = "Year",
     ylab = "Number of Passengers",
     col = "blue",
     lwd = 2)
```

### Forecast Accuracy

```{r}
# Check model accuracy
accuracy(arima_model)
```

**Key Metrics:**
- **RMSE**: Root Mean Squared Error (lower is better)
- **MAE**: Mean Absolute Error (lower is better)
- **MAPE**: Mean Absolute Percentage Error (lower is better)

### Enhanced Visualization

```{r}
# Convert to data frame for ggplot2
forecast_df <- data.frame(
  Date = as.Date(time(arima_forecast$mean)),
  Forecast = as.numeric(arima_forecast$mean),
  Lower80 = as.numeric(arima_forecast$lower[, 1]),
  Upper80 = as.numeric(arima_forecast$upper[, 1]),
  Lower95 = as.numeric(arima_forecast$lower[, 2]),
  Upper95 = as.numeric(arima_forecast$upper[, 2])
)

# Plot with ggplot2
autoplot(arima_forecast) +
  labs(
    title = "ARIMA Forecast: Airline Passengers",
    x = "Year",
    y = "Number of Passengers"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

## Part 4: Model Comparison and Selection (10 mins)

### Comparing Models

```{r}
# Linear Regression comparison
model1 <- lm(mpg ~ wt, data = mtcars)
model2 <- lm(mpg ~ wt + hp, data = mtcars)
model3 <- lm(mpg ~ wt + hp + am, data = mtcars)

# Compare R-squared and Adjusted R-squared
comparison <- data.frame(
  Model = c("wt only", "wt + hp", "wt + hp + am"),
  R_squared = c(
    summary(model1)$r.squared,
    summary(model2)$r.squared,
    summary(model3)$r.squared
  ),
  Adj_R_squared = c(
    summary(model1)$adj.r.squared,
    summary(model2)$adj.r.squared,
    summary(model3)$adj.r.squared
  ),
  AIC = c(AIC(model1), AIC(model2), AIC(model3))
)

comparison
```

### Model Selection Criteria

**For Linear Regression:**
- **R¬≤**: Higher is better (but can inflate with more predictors)
- **Adjusted R¬≤**: Penalizes for extra predictors
- **AIC**: Lower is better

**For Logistic Regression:**
- **AIC**: Lower is better
- **Deviance**: Lower is better
- **Accuracy**: Higher is better

**For Time Series:**
- **AIC/BIC**: Lower is better
- **RMSE/MAE**: Lower is better

------------------------------------------------------------------------

## Part 5: Practice and Q&A (10 mins)

### Practice Tasks

#### Task 1: Linear Regression
Predict `disp` (displacement) using `hp`, `wt`, and `cyl` in mtcars.

```{r}
# Solution
model_disp <- lm(disp ~ hp + wt + cyl, data = mtcars)
summary(model_disp)

# Visualization
ggplot(mtcars, aes(x = wt, y = disp, color = factor(cyl))) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    title = "Displacement vs Weight by Cylinders",
    x = "Weight (1000 lbs)",
    y = "Displacement (cu.in.)",
    color = "Cylinders"
  ) +
  theme_minimal()
```

#### Task 2: Logistic Regression Comparison
Build a simpler model using only `Income` and `Age`. Compare AIC with the full model.

```{r}
# Solution
log_reg_simple <- glm(marriedTarget ~ Income + Age, 
                      family = binomial(link = "logit"), 
                      data = credit_data)

# Compare AICs
cat("Full Model AIC:", AIC(log_reg), "\n")
cat("Simple Model AIC:", AIC(log_reg_simple), "\n")
cat("Difference:", AIC(log_reg) - AIC(log_reg_simple), "\n")

# Which is better?
if(AIC(log_reg) < AIC(log_reg_simple)) {
  cat("Full model is better (lower AIC)")
} else {
  cat("Simple model is better (lower AIC)")
}
```

#### Task 3: ARIMA with Nile Dataset
Forecast Nile river flow for next 5 years.

```{r}
# Solution
data(Nile)

# Plot original data
plot(Nile, 
     main = "Annual Nile River Flow",
     ylab = "Flow",
     xlab = "Year")

# Fit ARIMA model
nile_arima <- auto.arima(Nile)
summary(nile_arima)

# Forecast 5 years
nile_forecast <- forecast(nile_arima, h = 5)

# Plot forecast
plot(nile_forecast,
     main = "Nile River Flow Forecast",
     xlab = "Year",
     ylab = "Flow")
```

#### Task 4: Multiple Predictors
Using mtcars, find the best model to predict `mpg` by testing different combinations of predictors.

```{r}
# Solution
models <- list(
  m1 = lm(mpg ~ wt, data = mtcars),
  m2 = lm(mpg ~ wt + hp, data = mtcars),
  m3 = lm(mpg ~ wt + hp + am, data = mtcars),
  m4 = lm(mpg ~ wt + hp + am + cyl, data = mtcars),
  m5 = lm(mpg ~ wt + hp + am + cyl + disp, data = mtcars)
)

# Compare models
model_comparison <- data.frame(
  Model = paste0("Model ", 1:5),
  Predictors = c("wt", "wt+hp", "wt+hp+am", "wt+hp+am+cyl", "wt+hp+am+cyl+disp"),
  R2 = sapply(models, function(x) summary(x)$r.squared),
  Adj_R2 = sapply(models, function(x) summary(x)$adj.r.squared),
  AIC = sapply(models, AIC)
)

model_comparison

# Identify best model
best_model <- which.min(model_comparison$AIC)
cat("\nBest model based on AIC:", model_comparison$Predictors[best_model])
```

### Challenge: Complete Analysis Pipeline

Perform a complete regression analysis:
1. Load the `iris` dataset
2. Predict `Sepal.Length` using other numeric variables
3. Check model assumptions
4. Make predictions for new data
5. Create visualizations

```{r}
# Solution

# 1. Load data
data(iris)

# 2. Build model
iris_model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, 
                 data = iris)
summary(iris_model)

# 3. Check assumptions
par(mfrow = c(2, 2))
plot(iris_model)

# 4. Make predictions
new_flowers <- data.frame(
  Sepal.Width = c(3.0, 3.5, 4.0),
  Petal.Length = c(4.0, 5.0, 6.0),
  Petal.Width = c(1.2, 1.8, 2.2)
)

predictions <- predict(iris_model, newdata = new_flowers, interval = "confidence")
cbind(new_flowers, predictions)

# 5. Visualizations
par(mfrow = c(1, 2))

# Actual vs Predicted
plot(iris$Sepal.Length, fitted(iris_model),
     main = "Actual vs Predicted Sepal Length",
     xlab = "Actual",
     ylab = "Predicted",
     pch = 19,
     col = iris$Species)
abline(0, 1, col = "red", lwd = 2)

# Residuals
plot(fitted(iris_model), residuals(iris_model),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 19,
     col = iris$Species)
abline(h = 0, col = "red", lwd = 2)
```

------------------------------------------------------------------------

## üìö Key Takeaways

‚úÖ **Linear Regression** predicts continuous outcomes using one or more predictors  
‚úÖ **Coefficients** show direction and magnitude of relationships  
‚úÖ **R¬≤** measures proportion of variance explained (0 to 1)  
‚úÖ **Logistic Regression** predicts binary outcomes (0/1, Yes/No)  
‚úÖ **Odds Ratios** interpret logistic regression coefficients (exp(Œ≤))  
‚úÖ **ARIMA** forecasts time series based on past patterns  
‚úÖ **ARIMA(p,d,q)** parameters: AR terms, differences, MA terms  
‚úÖ **Lower AIC** indicates better model fit  
‚úÖ Always check model assumptions with diagnostic plots  
‚úÖ Visualize predictions to understand model behavior  

------------------------------------------------------------------------

## üéØ Model Selection Guide

| **Outcome Type** | **Model to Use** | **Key Metric** |
|-----------------|------------------|----------------|
| Continuous (numeric) | Linear Regression | R¬≤, Adj R¬≤, AIC |
| Binary (0/1, Yes/No) | Logistic Regression | AIC, Accuracy, Deviance |
| Time Series | ARIMA | AIC, RMSE, MAE |
| Count Data | Poisson Regression | AIC, Deviance |

------------------------------------------------------------------------

## üìñ Resources

-   Linear Regression: https://www.statmethods.net/stats/regression.html
-   Logistic Regression: https://www.statmethods.net/advstats/glm.html
-   ARIMA Tutorial: https://otexts.com/fpp2/arima.html
-   R Documentation: `?lm`, `?glm`, `?auto.arima`
-   Forecast Package: https://pkg.robjhyndman.com/forecast/

------------------------------------------------------------------------

## üîú Next Steps

**Advanced Topics to Explore:**
- Multiple regression with interaction terms
- Polynomial regression
- Ridge and LASSO regularization
- Cross-validation techniques
- Machine learning models (Random Forest, XGBoost)
- Advanced time series (SARIMA, Prophet)

**Recommended Practice:**
- Kaggle regression competitions
- Real-world datasets from UCI Machine Learning Repository
- R for Data Science book (Wickham & Grolemund)
